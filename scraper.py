import requests
import xml.etree.ElementTree as ET
import csv
import hashlib
import os
from datetime import datetime


class RedditScraper:

    def __init__(self, csv_file="posts.csv", subreddits=None):
        self.csv_file = csv_file
        self.subreddits = subreddits or ["laptop"]


    def generate_unique_id(self, text):
        return hashlib.md5(text.encode()).hexdigest()[:12]


    def scrape_subreddit(self, subreddit):
        url = f"https://www.reddit.com/r/{subreddit}/.rss"
        headers = {
            "User-Agent": (
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/123.0.0.0 Safari/537.36"
            ),
            "Accept": "application/rss+xml, application/xml;q=0.9,*/*;q=0.8"
        }

        try:
            response = requests.get(url, headers=headers, timeout=10)
            print("Status:", response.status_code)
        except requests.RequestException as e:
            print(f"‚ùå Request failed: {e}")
            return []

        posts = []

        if not response.text.strip().startswith("<"):
            print("‚ùå RSS blocked ‚Äî got HTML instead of XML.")
            return posts

        try:
            root = ET.fromstring(response.content)
        except ET.ParseError:
            print("‚ùå XML parsing failed.")
            return posts

        ns = {'atom': 'http://www.w3.org/2005/Atom'}

        for entry in root.findall('atom:entry', ns):
            title_el = entry.find('atom:title', ns)
            link_el = entry.find('atom:link', ns)
            published_el = entry.find('atom:published', ns)

            if title_el is None or link_el is None:
                continue

            title = title_el.text
            link = link_el.attrib.get("href")
            published = published_el.text if published_el is not None else "N/A"

            unique_id = self.generate_unique_id(link)

            posts.append({
                "topic": subreddit,
                "title": title,
                "link": link,
                "published_time": published,
                "unique_id": unique_id
            })

        return posts


    def scrape_all(self):
        combined = []
        for sub in self.subreddits:
            combined.extend(self.scrape_subreddit(sub))
        return combined


    def get_existing_data(self):
        existing_ids = set()
        existing_rows = []

        if not os.path.exists(self.csv_file):
            return existing_ids, existing_rows

        try:
            with open(self.csv_file, 'r', encoding='utf-8', newline='') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    if 'Unique_ID' in row:
                        existing_ids.add(row['Unique_ID'])
                        existing_rows.append(row)
        except Exception as e:
            print(f"‚ö†Ô∏è Error reading CSV: {e}")

        return existing_ids, existing_rows


    def save_posts_to_csv(self, new_posts):
        existing_ids, existing_rows = self.get_existing_data()

        # Filter out duplicates
        posts_to_add = [p for p in new_posts if p["unique_id"] not in existing_ids]

        if not posts_to_add:
            print("\nüìä No new posts to add")
            return

        # Create CSV if it doesn't exist
        file_exists = os.path.exists(self.csv_file)

        try:
            with open(self.csv_file, 'a', encoding='utf-8', newline='') as f:
                fieldnames = ["Topic", "Title", "Link", "Published_Time", "Unique_ID"]
                writer = csv.DictWriter(f, fieldnames=fieldnames)

                # Write header if file is new
                if not file_exists:
                    writer.writeheader()

                for p in posts_to_add:
                    writer.writerow({
                        "Topic": p["topic"],
                        "Title": p["title"],
                        "Link": p["link"],
                        "Published_Time": p["published_time"],
                        "Unique_ID": p["unique_id"]
                    })
                    print(f"‚ûï Added: {p['title'][:60]}...")

            print(f"\nüìä Added {len(posts_to_add)} new posts")

        except Exception as e:
            print(f"‚ùå Error writing to CSV: {e}")


    def clean_duplicates(self):
        if not os.path.exists(self.csv_file):
            print("CSV file doesn't exist yet.")
            return

        existing_ids, existing_rows = self.get_existing_data()

        seen = set()
        unique_rows = []

        for row in existing_rows:
            uid = row.get('Unique_ID', '')
            if uid and uid not in seen:
                seen.add(uid)
                unique_rows.append(row)

        duplicates_removed = len(existing_rows) - len(unique_rows)

        if duplicates_removed > 0:
            try:
                with open(self.csv_file, 'w', encoding='utf-8', newline='') as f:
                    fieldnames = ["Topic", "Title", "Link", "Published_Time", "Unique_ID"]
                    writer = csv.DictWriter(f, fieldnames=fieldnames)
                    writer.writeheader()
                    writer.writerows(unique_rows)

                print(f"‚úÖ Removed {duplicates_removed} duplicate rows")
            except Exception as e:
                print(f"‚ùå Error cleaning duplicates: {e}")
        else:
            print("‚úÖ No duplicates found")


    def run(self):
        print("\nüöÄ Starting Reddit ‚Üí CSV Sync...")
        print(f"‚è∞ Run time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')}\n")

        posts = self.scrape_all()

        if posts:
            self.save_posts_to_csv(posts)
        else:
            print("\n‚ö†Ô∏è No posts scraped")

        self.clean_duplicates()

        print("\n‚úÖ Sync completed successfully!\n")


if __name__ == "__main__":
    scraper = RedditScraper(
        csv_file="posts.csv",
        subreddits = ["SuggestALaptop","LaptopDeals","laptops","buildapcforme","studentlaptops","techsupport"]
    )
    
    scraper.run()


